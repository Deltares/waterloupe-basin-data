{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Imports\r\n",
    "from pathlib import Path\r\n",
    "import pandas as pd\r\n",
    "import json\r\n",
    "import csv\r\n",
    "import geopandas as gpd"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# List of colors to use in the timeseries (now only three, if you have more then 3 plots in 1 csv, please add some colors here!)\r\n",
    "colors = [\"#0178BE\", \"#B72025\", \"#75bbfd\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def xlsx2json(filename, result_folder, plot_type=\"timeseries\"):\r\n",
    "    # Converts a csv file into multiple json files. Each row represents\r\n",
    "    # a timeseries to be saved to a seperate json file\r\n",
    "    shortname = filename.stem\r\n",
    "    filename = filename.resolve()\r\n",
    "    print(shortname)\r\n",
    "\r\n",
    "\r\n",
    "    template = 'line_chart_template'\r\n",
    "    # To create a line chart, there is a template available\r\n",
    "    with open(f'{template}.json') as json_data:\r\n",
    "        template = json.load(json_data)\r\n",
    "\r\n",
    "    dfs = []\r\n",
    "\r\n",
    "    with open(filename, 'r') as csvfile:\r\n",
    "        nrows = (len(csvfile.readlines()))\r\n",
    "    with open(filename, 'r') as csvfile:\r\n",
    "        # the delimiter depends on how your CSV seperates values\r\n",
    "        csvReader = csv.reader(csvfile, delimiter=';')\r\n",
    "        data = []\r\n",
    "        for index, row in enumerate(csvReader):\r\n",
    "            if (index == nrows - 1):\r\n",
    "                data.append(row)\r\n",
    "            # check if row is empty\r\n",
    "            if (not any(row)) or (index == nrows - 1):\r\n",
    "                column = data[0][1]\r\n",
    "                column_names = data.pop(0)\r\n",
    "                df = pd.DataFrame(data, columns=column_names)\r\n",
    "                df = df.set_index(column)\r\n",
    "                dfs.append(df)\r\n",
    "                data = []\r\n",
    "            else:\r\n",
    "                data.append(row)\r\n",
    "\r\n",
    "    template['data']['labels'] = list(dfs[0].columns.values[1:])\r\n",
    "    for name in dfs[0].index:\r\n",
    "        new_data = template.copy()\r\n",
    "        empty_ds = new_data['data']['datasets'][0].copy()\r\n",
    "        new_data['data']['datasets'] = []\r\n",
    "        for index, df in enumerate(dfs):\r\n",
    "            empty_ds = empty_ds.copy()\r\n",
    "            label = df.columns[0]\r\n",
    "            df = df.iloc[:, 1:]\r\n",
    "            data = df.loc[name, :].values\r\n",
    "            # TODO: this is due to inconsistency in 1000 seperators in the csv files..\r\n",
    "            data = [float(str(d).replace('.', '')) for d in data]\r\n",
    "            data = [float(str(d).replace(',', '.')) for d in data]\r\n",
    "            empty_ds['data'] = list(data)\r\n",
    "            empty_ds['label'] = f'{label} {name}'\r\n",
    "            empty_ds['backgroundColor'] = colors[index]\r\n",
    "            empty_ds['borderColor'] = colors[index]\r\n",
    "\r\n",
    "            new_data['data']['datasets'].append(empty_ds)\r\n",
    "\r\n",
    "        output_folder = f'{result_folder}/{plot_type}/{name}'\r\n",
    "        Path(output_folder).mkdir(parents=True, exist_ok=True)\r\n",
    "\r\n",
    "        with open(f'{output_folder}/{shortname}.json', 'w+') as outfile:\r\n",
    "            json.dump(new_data, outfile)\r\n",
    "\r\n",
    "\r\n",
    "def get_overview_graph_files(data_folder):\r\n",
    "    folders = Path(data_folder).rglob(\"*\")\r\n",
    "    files = [f.resolve() for f in folders]\r\n",
    "    return files\r\n"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Define the folder where the csv files are stored for the timeseries\r\n",
    "data_folder = r'N:/Projects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries'\r\n",
    "result_folder = '../'\r\n",
    "\r\n",
    "# First get an overview of all the files living in the input folder\r\n",
    "data_overview = get_overview_graph_files(data_folder)\r\n",
    "\r\n",
    "# Loop over the files and create the line charts for each file for each area (area = defined in the file)\r\n",
    "for filename in data_overview:\r\n",
    "    xlsx2json(filename, result_folder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Agriculture Demand hist.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Availability Groundwater hist.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Availability Surface Water hist.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Domestic Demand hist.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future agriculture demand SSP2.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future agriculture demand SSP3.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future availability groundwater.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future availability surface.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future domestic demand SSP2.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future domestic demand SSP3.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future industry demand SSP2.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Future industry demand SSP3.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Industrial Demand hist.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Water Gap hist.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Water Gap SSP2.csv'), WindowsPath('//storage002.directory.intra/winprojects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/timeseries/Water Gap SSP3.csv')]\n",
      "Agriculture Demand hist\n",
      "Availability Groundwater hist\n",
      "Availability Surface Water hist\n",
      "Domestic Demand hist\n",
      "Future agriculture demand SSP2\n",
      "Future agriculture demand SSP3\n",
      "Future availability groundwater\n",
      "Future availability surface\n",
      "Future domestic demand SSP2\n",
      "Future domestic demand SSP3\n",
      "Future industry demand SSP2\n",
      "Future industry demand SSP3\n",
      "Industrial Demand hist\n",
      "Water Gap hist\n",
      "Water Gap SSP2\n",
      "Water Gap SSP3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "import matplotlib as mpl\r\n",
    "from matplotlib.colors import ListedColormap\r\n",
    "\r\n",
    "# This code cell is meant to create the color scale for the risk maps\r\n",
    "vmin = 0\r\n",
    "vmax = 1\r\n",
    "cmap = ListedColormap(['#2B663C', '#489557', '#7CBA6D', '#B1D678', '#DDED97', '#FAE096', '#F2B06E', '#E4744F', '#E4744F', '#971D2B'])\r\n",
    "\r\n",
    "norm = mpl.colors.Normalize(vmin=vmin, vmax=vmax)\r\n",
    "c = cmap(norm(10),bytes=True)\r\n",
    "\r\n",
    "def get_color(value):\r\n",
    "    c = cmap(norm(value),bytes=True)\r\n",
    "    return '#{:02x}{:02x}{:02x}'.format(c[0], c[1], c[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def shape2geojson(shapefile, datafile, result_folder, common_attr):\r\n",
    "  # This functrion reads out the shapefile as a  geopandas frame and combines it with an csv (which is converted\r\n",
    "  # by pandas into a dataframe) a common attribute is needed to know which dataframe column is supposed to be added \r\n",
    "  # to which polygon. If no column has the same name, reuse the 1th column\r\n",
    "  shortname = datafile.stem\r\n",
    "  filename = datafile.resolve()\r\n",
    "  print(shortname)\r\n",
    "\r\n",
    "  gdf = gpd.read_file(shapefile)\r\n",
    "  gdf = gdf.to_crs(\"EPSG:4326\")\r\n",
    "  df = pd.read_csv(filename, delimiter=';')\r\n",
    "  df = df.iloc[:, 1:]\r\n",
    "  for column in df.columns[1:]:\r\n",
    "      colors = [get_color(val) for val in df[column].values]\r\n",
    "      df[f'c_{column}'] = colors\r\n",
    "  df['popupHTML'] = [\"The risk for {name} is {value}.\" for x in range(len(df))]\r\n",
    "  df['color'] = [\"#000\" for x in range(len(df))]\r\n",
    "  df['fillOpacity'] = [1 for x in range(len(df))]\r\n",
    "\r\n",
    "  # TODO: now assuming the 1st column is the column to merge into the geojson\r\n",
    "  df.rename( columns={df.columns[0]: common_attr}, inplace=True )\r\n",
    "  df['name'] = df[common_attr].values\r\n",
    "\r\n",
    "  for name in df[common_attr]:\r\n",
    "      mergeddf  = gdf.merge(df, on=common_attr, how='left')\r\n",
    "      shape_file = gpd.GeoDataFrame(mergeddf)\r\n",
    "\r\n",
    "      output_folder = f'{result_folder}/maps/{name}'\r\n",
    "      Path(output_folder).mkdir(parents=True, exist_ok=True)\r\n",
    "      shape_file.to_file(f'{output_folder}/{shortname}.geojson', driver='GeoJSON')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "# Define the folder where the csv files are stored for the map data\r\n",
    "data_folder = r'N:/Projects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/maps/'\r\n",
    "result_folder = '../'\r\n",
    "shapefile = r\"D:/voorCindy/shp/Subcuencas_Chirilu_selected_wgs84_18s.shp\"\r\n",
    "common_attr = 'Nom_Senamh'\r\n",
    "\r\n",
    "data_overview = get_overview_graph_files(data_folder)\r\n",
    "for filename in data_overview:\r\n",
    "    shape2geojson(shapefile, filename, result_folder, common_attr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Water Gap Score fut sector SSP2\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "type numpy.ndarray doesn't define __round__ method",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0585aa557cb3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdata_overview\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_overview_graph_files\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_overview\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mshape2geojson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshapefile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcommon_attr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-222bf1342151>\u001b[0m in \u001b[0;36mshape2geojson\u001b[1;34m(shapefile, datafile, result_folder, common_attr)\u001b[0m\n\u001b[0;32m     13\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mcolors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mget_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m       \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m       \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34mf'c_{column}'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m   \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'popupHTML'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"The risk for {name} is {value}.\"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: type numpy.ndarray doesn't define __round__ method"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "source": [
    "# IGNORE \r\n",
    "# def csv2scatter(filename, result_folder):\r\n",
    "#     # Converts a csv file into multiple json files. Each row represents\r\n",
    "#     # a timeseries to be saved to a seperate json file\r\n",
    "#     shortname = filename.stem\r\n",
    "#     filename = filename.resolve()\r\n",
    "#     print(shortname)\r\n",
    "\r\n",
    "#     # To create a line chart, there is a template available\r\n",
    "#     with open('scatter_datalabels_template.json') as json_data:\r\n",
    "#         template = json.load(json_data)\r\n",
    "\r\n",
    "#     dfs = []\r\n",
    "\r\n",
    "#     with open(filename, 'r') as csvfile:\r\n",
    "#         nrows = (len(csvfile.readlines()))\r\n",
    "#     with open(filename, 'r') as csvfile:\r\n",
    "#         # the delimiter depends on how your CSV seperates values\r\n",
    "#         csvReader = csv.reader(csvfile, delimiter=';')\r\n",
    "#         data = []\r\n",
    "#         for index, row in enumerate(csvReader):\r\n",
    "#             if (index == nrows - 1):\r\n",
    "#                 data.append(row)\r\n",
    "#             # check if row is empty\r\n",
    "#             if (not any(row)) or (index == nrows - 1):\r\n",
    "#                 column = data[0][1]\r\n",
    "#                 column_names = data.pop(0)\r\n",
    "#                 df = pd.DataFrame(data, columns=column_names)\r\n",
    "#                 df = df.set_index(column)\r\n",
    "#                 dfs.append(df)\r\n",
    "#                 data = []\r\n",
    "#             else:\r\n",
    "#                 data.append(row)\r\n",
    "\r\n",
    "#     template['data']['labels'] = list(dfs[0].columns.values[1:])\r\n",
    "#     for name in dfs[0].index:\r\n",
    "#         new_data = template.copy()\r\n",
    "#         empty_ds = new_data['data']['datasets'][0].copy()\r\n",
    "#         new_data['data']['datasets'] = []\r\n",
    "#         for index, df in enumerate(dfs):\r\n",
    "#             empty_ds = empty_ds.copy()\r\n",
    "#             label = df.columns[0]\r\n",
    "#             df = df.iloc[:, 1:]\r\n",
    "#             data = df.loc[name, :].values\r\n",
    "#             # TODO: this is due to inconsistency in 1000 seperators in the csv files..\r\n",
    "#             data = [float(str(d).replace('.', '')) for d in data]\r\n",
    "#             data = [float(str(d).replace(',', '.')) for d in data]\r\n",
    "#             empty_ds['data'] = list(data)\r\n",
    "#             empty_ds['label'] = f'{label} {name}'\r\n",
    "#             empty_ds['backgroundColor'] = colors[index]\r\n",
    "#             empty_ds['borderColor'] = colors[index]\r\n",
    "\r\n",
    "#             new_data['data']['datasets'].append(empty_ds)\r\n",
    "\r\n",
    "#         output_folder = f'{result_folder}/timeseries/{name}'\r\n",
    "#         Path(output_folder).mkdir(parents=True, exist_ok=True)\r\n",
    "\r\n",
    "#         with open(f'{output_folder}/{shortname}.json', 'w+') as outfile:\r\n",
    "#             json.dump(new_data, outfile)\r\n",
    "\r\n",
    "# # Define the folder where the csv files are stored for the map data\r\n",
    "# data_folder = r'N:/Projects/1230000/1230409/B. Measurements and calculations/9. Site in Peru/dashboard/data/csv/maps/'\r\n",
    "# result_folder = '../'\r\n",
    "# shapefile = r\"D:/voorCindy/shp/Subcuencas_Chirilu_selected_wgs84_18s.shp\"\r\n",
    "# common_attr = 'Nom_Senamh'\r\n",
    "\r\n",
    "# data_overview = get_overview_graph_files(data_folder)\r\n",
    "# for filename in data_overview:\r\n",
    "#     csv2scatter(shapefile, filename, result_folder, common_attr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Water Gap Score fut sector SSP2\n",
      "Water Gap Score fut sector SSP3\n",
      "Water Gap Score future SSP2\n",
      "Water Gap Score future SSP3\n",
      "Water Gap Score sector\n",
      "Water Gap score\n",
      "Water scarcity Index (WSI) SSP2\n",
      "Water scarcity Index (WSI) SSP3\n",
      "Water Scarcity Index\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('py37': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "interpreter": {
   "hash": "6a1935f73f73fc0be28916cb521f6f52ddcedce212d0312141a48b20dd88a985"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}